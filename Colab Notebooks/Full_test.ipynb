{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPH41a6+xXo47uWA0wn7+og"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7y5x8IVNiPo","executionInfo":{"status":"ok","timestamp":1670737770974,"user_tz":-540,"elapsed":20266,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}},"outputId":"ddc356a1-8aae-49be-d9bc-0e9770c439a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/gdrive')\n"]},{"cell_type":"code","source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","from pandas.core.common import flatten\n","import copy\n","import numpy as np\n","import random\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import Dataset, DataLoader\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","\n","import glob\n","from tqdm import tqdm"],"metadata":{"id":"h5OEElDlNoCJ","executionInfo":{"status":"ok","timestamp":1670737778131,"user_tz":-540,"elapsed":7162,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# !pip install kora -q\n","# from kora import drive\n","# drive.link_nbs()a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxkV7c9jNoc7","executionInfo":{"status":"ok","timestamp":1670737572532,"user_tz":-540,"elapsed":10208,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}},"outputId":"9edfa5a2-a01e-4f26-e86c-a129a40561c3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 3.5 MB/s \n","\u001b[?25hMounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install import-ipynb\n","import import_ipynb"],"metadata":{"id":"5DdnCXEINqPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd \"/gdrive/MyDrive/Colab Notebooks/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Eg7RcWty_0p","executionInfo":{"status":"ok","timestamp":1670737811821,"user_tz":-540,"elapsed":515,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}},"outputId":"67b54e3c-e5f6-4ed1-d67a-cfa688669682"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["#@title Test for type\n","# Due to errors in importing, I copied the model here as well.\n","\n","\n","# ##############################\n","# below is the code for the inception block\n","from torch.nn.modules.batchnorm import BatchNorm2d\n","class InceptionBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(InceptionBlock, self).__init__()\n","\n","        assert out_channels%8==0, 'out channel should be mutiplier of 8'\n","\n","        ################################\n","        ## P4.1. Write your code here ##\n","        self.block_1 = nn.Sequential(\n","            nn.Conv2d(in_channels,out_channels//4,kernel_size=1,padding=0,stride=1,bias=False),\n","            BatchNorm2d(out_channels//4),\n","            nn.ReLU()\n","        )\n","        self.block_2= nn.Sequential(\n","            nn.Conv2d(in_channels,out_channels//2,kernel_size=1,padding=0,stride=1,bias=False),\n","            BatchNorm2d(out_channels//2),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels//2,out_channels//2, stride=1,padding=1,kernel_size=3,bias=False),\n","            BatchNorm2d(out_channels//2),\n","            nn.ReLU()\n","        )\n","        self.block_3 = nn.Sequential(\n","            nn.Conv2d(in_channels,out_channels//8,kernel_size=1,padding=0,stride=1,bias=False),\n","            BatchNorm2d(out_channels//8),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels//8,out_channels//8, stride=1,padding=2,kernel_size=5,bias=False),\n","            BatchNorm2d(out_channels//8),\n","            nn.ReLU()\n","        )\n","        self.max_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3,stride=1,padding=1),\n","            nn.Conv2d(in_channels,out_channels//8,kernel_size=1,padding=0,stride=1,bias=False),\n","            BatchNorm2d(out_channels//8),\n","            nn.ReLU()\n","        )\n","\n","\n","        ################################\n","\n","    def forward(self, x):\n","\n","        ################################\n","        ## P4.2. Write your code here ##\n","        output1= self.block_1(x)\n","        output2= self.block_2(x)\n","        output3= self.block_3(x)\n","        output4= self.max_pool(x)\n","        # print(\"output1 size {}, output2 size {}, output3 size {}, output4 size {}, \".format(output1.size(),output2.size(),output3.size(),output4.size()))\n","        output = torch.cat( (self.block_1(x),self.block_2(x),self.block_3(x),self.max_pool(x)), 1 )\n","\n","\n"," \n","        return output\n","        ################################\n","# Below is our code for the inception model, which consists of inception blocks defined above and other layers\n","\n","from torch.nn.modules.flatten import Flatten\n","from torch.nn.modules.pooling import MaxPool2d\n","class MyNetwork(nn.Module):\n","    def __init__(self, nf, block_type='inception', num_blocks=[1,1, 1, 1,1]):\n","        super(MyNetwork, self).__init__()\n","\n","        \n","        self.block_type = block_type\n","\n","        # Define blocks according to block_type\n","        if self.block_type == 'inception':\n","            block = InceptionBlock\n","            block_args = lambda x: (x, x)\n","\n","        # Define block layer by stacking multiple blocks. \n","        self.block_00 = nn.Sequential(*[block(*block_args(nf)) for _ in range(num_blocks[0])])\n","        self.block0 = nn.Sequential(*[block(*block_args(nf*2)) for _ in range(num_blocks[1])])\n","        self.block1 = nn.Sequential(*[block(*block_args(nf*4)) for _ in range(num_blocks[2])])\n","        self.block2 = nn.Sequential(*[block(*block_args(nf*4)) for _ in range(num_blocks[3])])\n","        self.block3 = nn.Sequential(*[block(*block_args(nf*4)) for _ in range(num_blocks[4])])\n","\n","        ################################\n","\n","        self.pre_block_00 = nn.Sequential(\n","            nn.Conv2d(3,nf,stride=1,kernel_size=3,padding=1,bias=False),\n","            nn.BatchNorm2d(nf),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,stride=2)\n","        \n","        )\n","\n","        self.pre_block0 = nn.Sequential(\n","            nn.Conv2d(nf,nf*2,stride=1,kernel_size=3,padding=1,bias=False),\n","            nn.BatchNorm2d(nf*2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,stride=2)\n","        \n","        )\n","\n","        self.pre_block1 = nn.Sequential(\n","            nn.Conv2d(nf*2,nf*4,stride=1,kernel_size=3,padding=1,bias=False),\n","            nn.BatchNorm2d(nf*4),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,stride=2)\n","        )\n","        self.pre_block2 = nn.Sequential(\n","            nn.Conv2d(nf*4,nf*4,stride=1,kernel_size=3,padding=1,bias=False),\n","            nn.BatchNorm2d(nf*4),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,stride=2)\n","        )\n","        self.pre_block3 = nn.Sequential(\n","            nn.Conv2d(nf*4,nf*4,stride=1,kernel_size=3,padding=1,bias=False),\n","            nn.BatchNorm2d(nf*4),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,stride=2)\n","        )\n","        self.final = nn.Sequential(\n","            nn.AdaptiveAvgPool2d( 1 ),\n","            nn.Flatten(),\n","            nn.Linear( nf*4 , 8)\n","        )\n","\n","\n","        ################################\n","\n","    def forward(self, x):\n","        # the forward function\n","        #######################################################################\n","        first_out = self.block_00(self.pre_block_00(x))\n","        output_0 = self.block0(self.pre_block0(first_out))\n","        output1_1=self.pre_block1(output_0)\n","        output1= self.block1(output1_1)\n","        output2=self.block2(self.pre_block2(output1))\n","        output3=self.block3(self.pre_block3(output2))\n","        output = self.final(output3)\n","        # print(\"size of tensor after final stage: \",output.size())\n","\n"," \n","        return output\n","        #######################################################################\n","\n","\n","\n","\n","# ##############################\n","# import Damage_type_self_inception\n","num_filters=16\n","num_blocks = [5,5,5,5,5]\n","model_type = MyNetwork(num_filters, 'inception', num_blocks)\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_path = '/gdrive/MyDrive/project/type_inception.pt'\n","# model_loaded = model.load_state_dict(torch.load(model_path))\n","\n","model_type.load_state_dict(torch.load(model_path))\n","model_type.cuda()\n","model_type.eval()\n","\n","import torchvision.transforms as transforms\n","\n","def type_preprocess(img):\n","  test_transform = transforms.Compose([\n","    transforms.CenterCrop(128),                       \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","    ])\n","\n","  return test_transform(img)\n","import urllib\n","\n","from PIL import Image\n","from urllib.request import Request, urlretrieve  # Python 3\n","\n","def type_test(image_path):\n","    \n","    urlretrieve(image_path, 'save.jpg') # or other way to upload image\n","    # Image.open('save.jpg')\n","    img = type_preprocess(Image.open('save.jpg'))\n","    img = img.to(device)\n","    img = img.unsqueeze(0)\n","\n","\n","    with torch.no_grad():\n","      output = model_type(img)\n","      # print(output)\n","      # print(output)\n","      if output[0].argmax()==0:\n","        return ' Prediction is: Broken headlamp '\n","      elif output[0].argmax()==1:\n","        return ' Prediction is: Broken tail lamp '\n","      elif output[0].argmax()==2:\n","        return ' Prediction is: glass shatter '\n","      elif output[0].argmax()==3:\n","        return ' Prediction is: door scratch '\n","      elif output[0].argmax()==4:\n","        return ' Prediction is: door dent '\n","      elif output[0].argmax()==5:\n","        return ' Prediction is: bumper dent '\n","      elif output[0].argmax()==6:\n","        return ' Prediction is: bumper scratch '\n","      else:\n","        return ' Prediction is: Unknown damge type '\n","\n"],"metadata":{"id":"f32XQ9H4PaKg","executionInfo":{"status":"ok","timestamp":1670737942386,"user_tz":-540,"elapsed":584,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}},"cellView":"form"},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#@title Test for severity\n","import import_ipynb\n","import Damage_severity_self_inception\n","\n","num_filters=16\n","num_blocks = [5,5,5,5,5]\n","model_severity = Damage_severity_self_inception.MyNetwork(num_filters, 'inception', num_blocks)\n","\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_path = '/gdrive/MyDrive/project/severity_inception.pt'\n","# model_loaded = model.load_state_dict(torch.load(model_path))\n","\n","model_severity.load_state_dict(torch.load(model_path))\n","model_severity.cuda()\n","model_severity.eval()\n","\n","import torchvision.transforms as transforms\n","\n","def severity_preprocess(img):\n","  test_transform = transforms.Compose([\n","    transforms.CenterCrop(128),                       \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","    ])\n","\n","  return test_transform(img)\n","\n","import urllib\n","\n","from PIL import Image\n","from urllib.request import Request, urlretrieve  # Python 3\n","\n","def severity_test(image_path):\n","    \n","    urlretrieve(image_path, 'save.jpg') # or other way to upload image\n","    # Image.open('save.jpg')\n","    img = severity_preprocess(Image.open('save.jpg'))\n","    img = img.to(device)\n","    img = img.unsqueeze(0)\n","\n","\n","    with torch.no_grad():\n","      output = model_severity(img)\n","      # print(output)\n","      # print(output)\n","      if output[0].argmax()==0:\n","        return ' Minor damage '\n","      elif output[0].argmax()==1:\n","        return ' Prediction is: Moderate damage '\n","      else:\n","        return ' Prediction is: Severe damage '\n","\n"],"metadata":{"cellView":"form","id":"rS9xzT-k1dhm","executionInfo":{"status":"ok","timestamp":1670739811135,"user_tz":-540,"elapsed":542,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["#@title Test for location_cifar\n","import import_ipynb\n","\n","import Damage_location_self_inception\n","\n","model_loc_cifar = Damage_location_self_inception.newnetwork(num_classes=3).to(device)\n","model_path = '/gdrive/MyDrive/project/car_damage_location/self_inception/test_inception.pt'\n","# model_loaded = model.load_state_dict(torch.load(model_path))\n","\n","model_loc_cifar.load_state_dict(torch.load(model_path))\n","model_loc_cifar.cuda()\n","model_loc_cifar.eval()\n","\n","location_test_transform = transforms.Compose([\n","    transforms.Resize(128),\n","    transforms.CenterCrop(32),                       \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","    ])\n","def location_preprocess(img):\n","  img_processed = location_test_transform(img)\n","  return img_processed\n","\n","# test\n","import urllib\n","\n","from PIL import Image\n","from urllib.request import Request, urlretrieve  # Python 3\n","\n","def test_loc_cifar(image_path):\n","    \n","    urlretrieve(image_path, 'save.jpg') # or other way to upload image\n","    # Image.open('save.jpg')\n","    img = location_preprocess(Image.open('save.jpg'))\n","    img = img.to(device)\n","    img = img.unsqueeze(0)\n","\n","    print('We are going to predict the location of damage with second location model:')\n","    with torch.no_grad():\n","      output = model_loc_cifar(img)\n","      # print(output)\n","      # print(output)\n","      if output[0].argmax()==0:\n","        return ' Front damage '\n","      elif output[0].argmax()==1:\n","        return ' Prediction is: Side damage '\n","      else:\n","        return ' Prediction is: Rear damage '\n"],"metadata":{"cellView":"form","id":"TExyexat2Obd","executionInfo":{"status":"ok","timestamp":1670739786481,"user_tz":-540,"elapsed":760,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["#@title Test for location_imagenet\n","import import_ipynb\n","device = torch.device(\"cuda\")\n","\n","import damage_location_2\n","\n","transform = transforms.Compose(\n","        [\n","            transforms.Resize((356, 356)),\n","            transforms.RandomCrop((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","# test\n","import urllib\n","import torch\n","from PIL import Image\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def preprocess(img):\n","  img = transform(img)\n","\n","  return img\n","\n","\n","path = '/gdrive/MyDrive/project/car_damage_location/model_location_weights_final_2.pth'\n","model_loc_imagenet = damage_location_2.inceptionv3()\n","\n","model_loc_imagenet.load_state_dict(torch.load(path))\n","model_loc_imagenet.to(device)\n","\n","import urllib\n","\n","from urllib.request import Request, urlretrieve  # Python 3\n","\n","def test_loc_imagenet(image_path):\n","    \n","    urlretrieve(image_path, 'save.jpg') # or other way to upload image\n","    # Image.open('save.jpg')\n","    img = transform(Image.open('save.jpg'))\n","    img = img.to(device)\n","    img = img.unsqueeze(0)\n","    model_loc_imagenet.eval()\n","    print('We are going to predict the location of damage with first location model:')\n","    with torch.no_grad():\n","      output = model_loc_imagenet(img)\n","      # print(output)\n","      # print(output)\n","      if output[0].argmax()==0:\n","        return ' Front damage '\n","      elif output[0].argmax()==1:\n","        return ' Prediction is: Side damage '\n","      else:\n","        return ' Prediction is: Rear damage '\n","\n","\n","\n","\n","\n"],"metadata":{"cellView":"form","id":"KSsobNhH3ex7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Test for damaged\n","import import_ipynb\n","import Car_damaged_or_not\n","device = torch.device(\"cuda\")\n","\n","\n","class CNN(nn.Module):\n","    def __init__(self, train_CNN=False, num_classes=1):\n","        super(CNN, self).__init__()\n","        self.train_CNN = train_CNN\n","        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n","        self.inception.dropout=nn.Dropout(0.3)\n","        self.inception.fc = nn.Linear(self.inception.fc.in_features, num_classes)\n","        \n","        self.dropout = nn.Dropout(0.4)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, images):\n","        features = self.inception(images)\n","        features_2 = self.dropout(features)\n","        return self.sigmoid(features_2).squeeze(1)\n","\n","\n","model_damaged = CNN()\n","model_path = '/gdrive/MyDrive/project/car_damaged_or_not/model_weights.pth'\n","model_damaged.load_state_dict(torch.load(model_path))\n","\n","model_damaged.load_state_dict(torch.load(model_path))\n","model_damaged.cuda()\n","model_damaged.eval()\n","\n","import torchvision.transforms as transforms\n","transform = transforms.Compose(\n","        [\n","            transforms.Resize((356, 356)),\n","            transforms.RandomCrop((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","# test\n","import urllib\n","import torch\n","from PIL import Image\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","from urllib.request import Request, urlretrieve  # Python 3\n","\n","def damaged_test(image_path):\n","    \n","    urlretrieve(image_path, 'save.jpg') # or other way to upload image\n","    # Image.open('save.jpg')\n","    img = transform(Image.open('save.jpg'))\n","    img = img.to(device)\n","    img = img.unsqueeze(0)\n","\n","    print('We are going to predict whether it is damaged!')\n","    with torch.no_grad():\n","      output = model_damaged(img)\n","      # print(output)\n","      if output[0] > 0.45 and output[0] < 0.55:\n","        return \"We are not sure about the car's condition\"\n","      elif output[0] > 0.55:\n","        return ' The car seems to be whole. Are you sure it is damaged? '\n","      else:\n","        return ' The car is damaged! We are certain in the damage '\n","    \n","\n"],"metadata":{"cellView":"form","id":"Txaorw8v2XYO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Test for car\n","\n","!pip install efficientnet_pytorch\n","\n","torch.backends.cudnn.benchmark = True\n","\n","!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n","import pydensecrf.densecrf as dcrf\n","import pydensecrf.utils as utils\n","import json\n","path = '/gdrive/MyDrive/project/'\n","import urllib\n","from efficientnet_pytorch import EfficientNet\n","model_car = EfficientNet.from_pretrained('efficientnet-b4')\n","from urllib.request import Request, urlretrieve  # Python 3\n","!wget https://raw.githubusercontent.com/lukemelas/EfficientNet-PyTorch/master/examples/simple/img.jpg\n","!wget https://raw.githubusercontent.com/lukemelas/EfficientNet-PyTorch/master/examples/simple/labels_map.txt\n","labels_map = json.load(open('labels_map.txt'))\n","labels_map = [labels_map[str(i)] for i in range(1000)]\n","# list of top classes\n","top_classes = json.load( open( path+\"car_detection.json\" ) )\n","\n","def car_preprocess(image):\n","  transform  = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","  processed_image = transform(image).unsqueeze(0)\n","  return processed_image\n","\n","def car_categories(image_path):\n","    \n","    urlretrieve(image_path, 'save.jpg') # or other way to upload image\n","    img = car_preprocess(Image.open('save.jpg'))\n","    model_car.eval()\n","    with torch.no_grad():\n","      output = model_car(img)\n","    top_preds = torch.topk(output, k=3).indices.squeeze(0).tolist()\n","    result = [ (labels_map[index], torch.softmax(output,dim=1)[0,index].item() ) for index in top_preds]\n","\n","    print ('Module 1: We are going to predict whether this is a car...')\n","    for category in result:\n","        # print(top)\n","        if category[0] in top_classes:\n","            print ('Our model has detected: ',category[0], 'with percentage:', category[1])\n","            return \"The model predicts this is a car\"\n","    return \"Not a car picture. Please try again.\"\n","\n"],"metadata":{"cellView":"form","id":"negDIbL_2Y8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def full_test(image_path):\n","  url = image_path\n","  car = car_categories(url)\n","  if car != 'The model predicts this is a car':\n","    return 'Testing stops here'\n","  else:\n","    print(car)\n","  print('\\n Going to the next test. Is the car Damaged? ')\n","  \n","  damaged = damaged_test(url)\n","  if damaged == ' The car seems to be whole. Are you sure it is damaged? ':\n","    return 'The car is not damaged. Testing stops here.' \n","  else:\n","    print(damaged)\n","  print('\\n Going to the next test.Location of Damage.')\n","\n","  location_1 = test_loc_imagenet(url)\n","  print(location_1)\n","  location_2 = test_loc_cifar(url)\n","  print(location_2)\n","  print('\\nGoing to the next test. Severity of Damage:')\n","\n","  severity = severity_test(url)\n","  print(severity)\n","  print('\\n Going to the last test.Type of Damage:')\n","\n","  type_damage = type_test(url)\n","  print(type_damage)\n"],"metadata":{"id":"cbsfX0ZL1TiL","executionInfo":{"status":"ok","timestamp":1670740503098,"user_tz":-540,"elapsed":3,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["url = 'https://cdn.motor1.com/images/mgl/N0n7Y/s1/2019-hyundai-elantra-sport-review.jpg'\n","full_test(url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"GHBXHXquz5NI","executionInfo":{"status":"ok","timestamp":1670740511704,"user_tz":-540,"elapsed":6832,"user":{"displayName":"Eugen Coroi","userId":"04782466448468484864"}},"outputId":"db279d96-465e-404d-f98b-e075064f8a55"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Module 1: We are going to predict whether this is a car...\n","Our model has detected:  car wheel with percentage: 0.5786836743354797\n","The model predicts this is a car\n","\n"," Going to the next test. Is the car Damaged? \n","We are going to predict whether it is damaged!\n"]},{"output_type":"execute_result","data":{"text/plain":["'The car is not damaged. Testing stops here.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":[],"metadata":{"id":"lLVSfcBI0eWE"},"execution_count":null,"outputs":[]}]}